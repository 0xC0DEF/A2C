{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import time\n",
    "import copy\n",
    "from collections import deque\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch as tc\n",
    "import torch.nn as nn\n",
    "import torch.optim as opt\n",
    "import torch.distributions as tcdist\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from model import SnakeNet\n",
    "from core import CUDA_AVAILABLE, DEVICE\n",
    "from config import NROW,NCOL\n",
    "from env import Env\n",
    "from util import state2input\n",
    "\n",
    "class ChartWindow:\n",
    "    def __init__(self,name):\n",
    "        self.name=name\n",
    "        self.data=deque(maxlen=1000)\n",
    "        self.maxval=-9999\n",
    "        self.minval=9999\n",
    "    def add(self,x):\n",
    "        self.data.append(x)\n",
    "        self.maxval=max(self.maxval,x)\n",
    "        self.minval=min(self.minval,x)\n",
    "    def draw(self,ax):\n",
    "        ax.plot(self.data)\n",
    "        ax.plot([self.maxval]*len(self.data))\n",
    "        ax.plot([self.minval]*len(self.data))\n",
    "        ax.set_title(self.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train Parameters\n",
    "\n",
    "BATCH_SIZE = 128\n",
    "DISC_RATIO=0.94\n",
    "GRADCLIP_NORM=1\n",
    "L2_DECAY=1e-6\n",
    "LEARNING_RATE=5e-6\n",
    "STAT_DISPLAY_FREQ = 50\n",
    "SAVE_TEMP_FREQ = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time as timelib\n",
    "from PIL import Image\n",
    "from PIL import ImageDraw\n",
    "from torchvision import transforms\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython import display\n",
    "def render(state,sleep_time=0,clear=True):\n",
    "    #print(obs)\n",
    "    txt = Image.new(\"RGBA\", (NCOL*30,NROW*30), (10,50,100,100))\n",
    "    draw = ImageDraw.Draw(txt)\n",
    "    grid_size = 30\n",
    "    for i in range(NROW):\n",
    "        for j in range(NCOL):\n",
    "            y1 = i*grid_size\n",
    "            x1 = j*grid_size\n",
    "            y2 = (i+1)*grid_size\n",
    "            x2 = (j+1)*grid_size\n",
    "            draw.rectangle(((x1, y1), (x2, y2)), outline='black', width=1)\n",
    "    for y,x in state['foods']:\n",
    "        draw.ellipse((x*grid_size, y*grid_size, (x+1)*grid_size, (y+1)*grid_size), fill = 'yellow', outline ='yellow')\n",
    "    for y,x in state['snake']:\n",
    "        if (y,x)==state['snake'][-1]:\n",
    "            draw.polygon([((x+1/2)*grid_size,y*grid_size),(x*grid_size,(y+1/2)*grid_size),\n",
    "                ((x+1/2)*grid_size,(y+1)*grid_size), ((x+1)*grid_size,(y+1/2)*grid_size)], fill = 'red')\n",
    "        else:\n",
    "            draw.rectangle(((x*grid_size, y*grid_size), ((x+1)*grid_size, (y+1)*grid_size)), fill='red', outline='red')\n",
    "    txt = txt.resize((64, 64*NROW//NCOL)).convert('RGB')\n",
    "    numpy_image = np.array(txt)\n",
    "    plt.axis(\"off\")\n",
    "    trans1 = transforms.ToTensor()\n",
    "    tensor_image = trans1(numpy_image)\n",
    "    tf = transforms.ToPILImage()\n",
    "    plt.imshow(tf(tensor_image))\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "    if sleep_time:\n",
    "        timelib.sleep(sleep_time)\n",
    "    if clear:\n",
    "        display.clear_output(wait=True)\n",
    "    return tensor_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "net_from = input('load network from(esc to new network)')\n",
    "net_to = input('save network to') or \"test.w\"\n",
    "\n",
    "print(\"CUDA: \",CUDA_AVAILABLE)\n",
    "net=SnakeNet().cuda() if CUDA_AVAILABLE else SnakeNet()\n",
    "if net_from!='':\n",
    "    net.load_state_dict(tc.load('./netw.pt'))\n",
    "net.train()\n",
    "\n",
    "opter=opt.Adam(net.parameters(),lr=LEARNING_RATE,weight_decay=L2_DECAY,eps=1e-3)\n",
    "#opter=opt.RMSprop(net.parameters(),lr=LEARNING_RATE,eps=1e-4,momentum=0.9)\n",
    "losses = []\n",
    "scores = []\n",
    "probmaxs = []\n",
    "\n",
    "def train(bat):\n",
    "    s1bat=tc.tensor([state2input(s1) for (s1,a,r,s2) in bat]).to(DEVICE)\n",
    "    abat=tc.tensor([a for (s1,a,r,s2) in bat]).to(DEVICE)\n",
    "    rbat=tc.tensor([r for (s1,a,r,s2) in bat]).to(DEVICE)\n",
    "    s2bat=tc.tensor([state2input(s2) for (s1,a,r,s2) in bat]).to(DEVICE)\n",
    "    dbat=tc.tensor([int(s2['done']) for (s1,a,r,s2) in bat]).to(DEVICE)\n",
    "\n",
    "    td_target=rbat+DISC_RATIO*((1-dbat)*net.calcval(s2bat).squeeze())\n",
    "    delta=td_target-net.calcval(s1bat).squeeze()\n",
    "\n",
    "    polraw = net.calcpol(s1bat)\n",
    "    pol_a = F.softmax(polraw,dim=1).gather(1,abat.unsqueeze(dim=1)).squeeze()\n",
    "    loss_val = ((net.calcval(s1bat).squeeze()-td_target.detach())**2).sum()\n",
    "    loss_pol = (-tc.log(pol_a) * delta.detach()).sum()\n",
    "    loss_entropy = (F.log_softmax(polraw,dim=1)*F.softmax(polraw,dim=1)).sum()\n",
    "    loss = loss_pol + 0.5*loss_val + 0.01*loss_entropy\n",
    "\n",
    "    opter.zero_grad()\n",
    "    loss.backward()\n",
    "    nn.utils.clip_grad_norm_(net.parameters(),GRADCLIP_NORM)\n",
    "    opter.step()\n",
    "    \n",
    "    losses.append(float(loss.mean()))\n",
    "\n",
    "train_i=0\n",
    "chart_lossavg=ChartWindow('Loss Avg')\n",
    "chart_losssd=ChartWindow('Loss SD')\n",
    "chart_maxprobavg=ChartWindow('Max Prob Avg')\n",
    "chart_scoremax=ChartWindow('Score Max')\n",
    "chart_scoreavg=ChartWindow('Score Avg')\n",
    "chart_scoremed=ChartWindow('Score Med')\n",
    "chart_scoremin=ChartWindow('Score Min')\n",
    "f = open(net_to+'.train_info', \"a\")\n",
    "\n",
    "def tmpsave():\n",
    "    tc.save(net.state_dict(),net_to)\n",
    "    display.clear_output(wait=True)\n",
    "    fig, (ax1,ax2,ax3,ax4) = plt.subplots(4,2)\n",
    "    #plt.figure(figsize=(2,2),dpi=50)\n",
    "    chart_lossavg.draw(ax1[0])\n",
    "    chart_losssd.draw(ax1[1])\n",
    "    chart_maxprobavg.draw(ax2[0])\n",
    "    chart_scoremax.draw(ax2[1])\n",
    "    chart_scoreavg.draw(ax3[0])\n",
    "    chart_scoremed.draw(ax3[1])\n",
    "    chart_scoremin.draw(ax4[0])\n",
    "    plt.show()\n",
    "\n",
    "def showinfo():\n",
    "    lossavg = sum(losses)/len(losses) if losses else -1\n",
    "    losssd = np.std(losses) if losses else -1\n",
    "    maxprobavg = sum(probmaxs)/len(probmaxs) if probmaxs else -1\n",
    "    scoremax = max(scores) if scores else -1\n",
    "    scoreavg=sum(scores)/len(scores) if scores else -1\n",
    "    scoremed=sorted(scores)[len(scores)//2] if scores else -1\n",
    "    scoremin=sorted(scores)[0] if scores else -1\n",
    "    print(\"#{}: LossAvg={:.3f}, LossSD={:.2f}, MaxprobAvg={:.3f}, ScoreMax={:.2f}, ScoreAvg={:.2f}, ScoreMed={:.2f}, ScoreMin={:.2f}\"\n",
    "        .format(train_i,lossavg,losssd,maxprobavg,scoremax,scoreavg,scoremed,scoremin))\n",
    "    chart_lossavg.add(lossavg)\n",
    "    chart_losssd.add(losssd)\n",
    "    chart_maxprobavg.add(maxprobavg)\n",
    "    chart_scoremax.add(scoremax)\n",
    "    chart_scoreavg.add(scoreavg)\n",
    "    chart_scoremed.add(scoremed)\n",
    "    chart_scoremin.add(scoremin)\n",
    "    print(lossavg,losssd,maxprobavg,scoremax,scoreavg,scoremed,scoremin,file=f)\n",
    "\n",
    "\n",
    "envs = [Env(True) for _ in range(BATCH_SIZE)]\n",
    "try:\n",
    "    while True:\n",
    "        train_i+=1\n",
    "        batch = []\n",
    "        for env in envs:\n",
    "            s1=env.state\n",
    "            if not s1 or s1['done']:\n",
    "                scores.append(env.score)\n",
    "                s1=env.reset()\n",
    "            # if env==envs[0]:\n",
    "            #     print(env.score)\n",
    "            #     render(s1,0,True)\n",
    "            pol = F.softmax(net.calcpol(tc.tensor(state2input(s1)).to(DEVICE)),dim=1)\n",
    "            probmaxs.append(float(tc.max(pol)))\n",
    "            a = tcdist.Categorical(pol).sample().item()\n",
    "            s2,rwd=env.step(a)\n",
    "            batch.append((s1,a,rwd,s2))\n",
    "        train(batch)\n",
    "        if train_i%SAVE_TEMP_FREQ==0:\n",
    "            tmpsave()\n",
    "        if train_i%STAT_DISPLAY_FREQ==0:\n",
    "            showinfo()\n",
    "            losses=[]\n",
    "            probmaxs=[]\n",
    "            scores=[]\n",
    "except KeyboardInterrupt:\n",
    "    tc.save(net.state_dict(),net_to)\n",
    "    print(\"Saved to\",net_to)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning rate가 1e4일때 nan가중치 생기는 문제있어서 1e5로 내렸더니 잘 된다.\n",
    "이유가 뭘까? loss func의 log에 들어가는 수가 오차때문에 0또는 음수되서 그런듯. 버그 재현해서 losses마지막에 nan들어가있다면 확실(아마도 확인했던듯)\n",
    "math.isnan(float(loss))체크해서 nan되기전에 loss가 먼저nan이면 더 확실\n",
    "https://www.kdnuggets.com/2017/08/37-reasons-neural-network-not-working.html/2 의 항목37이 도움되는듯\n",
    "\n",
    "loss가 nan이 되는게 아니라, loss표준편차를 보니 loss가 엄청 커져서 그렇다. 짐작으로는 수렴에 가까워져서 Adam의 momentum계산이 커져서 그런듯.\n",
    "SGD로 바꿔보자. adam eps를 키워보면 되려나?\n",
    "Adam학습속도 버릴수가 없다.\n",
    "원래 과학습되면 이렇고, 그래서 RL도 조기종료(early stopping)를 아무튼 해야하긴 하는듯.\n",
    "\n",
    "#TRAINCNT없애고 무한루프로 돌리자\n",
    "#지나쳐도 알 수 있게 performance 저장하고 저장주기마다 차트출력하자. matplotlib으로 그리는게 맞을듯\n",
    "#try catch로 keyboard exception만 잡아서 차트보여주고 훈련종료하게 하자\n",
    "#훈련 나눠서 여러번 돌릴 수 있도록, 훈련시작하면 불러올 가중치경로와 저장할 가중치이름 입력받도록 하자\n",
    "#가중치이름.info로 파일열어서 훈련정보 저장하도록 하자. 자동으로 버퍼 플러시 관리되니까 계속 출력하면 상관없을듯?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}